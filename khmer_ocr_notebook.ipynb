{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a12823a8",
   "metadata": {},
   "source": [
    "# Khmer and English OCR System Using CRAFT and TrOCR\n",
    "\n",
    "This notebook implements an end-to-end OCR system focused on Khmer and English languages using:\n",
    "1. CRAFT for Text Detection\n",
    "2. TrOCR for Text Recognition\n",
    "3. Synthetic data generation and augmentation for robust model training\n",
    "\n",
    "## Research Objectives\n",
    "- Create large amount dataset for training and testing text recognition\n",
    "- Develop a robust end-to-end OCR system for Khmer and English printed texts by leveraging synthetic data generation\n",
    "- Train models to handle real-world conditions (noise, distortion, complex backgrounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000734c3",
   "metadata": {},
   "source": [
    "## Setup and Environment Check\n",
    "\n",
    "First, let's check if we are running in Google Colab and set up the environment accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dab6273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Colab. Please install required packages manually if needed.\n"
     ]
    }
   ],
   "source": [
    "# Check if running in Google Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab! Installing required packages...\")\n",
    "    # Install required packages for Colab\n",
    "    !pip install -q torch torchvision\n",
    "    !pip install -q transformers\n",
    "    !pip install -q opencv-python\n",
    "    !pip install -q matplotlib\n",
    "    !pip install -q pillow\n",
    "    !pip install -q pandas\n",
    "    !pip install -q streamlit\n",
    "    !pip install -q datasets\n",
    "    !pip install -q huggingface_hub\n",
    "    \n",
    "    # Install Tesseract OCR\n",
    "    !apt-get update\n",
    "    !apt-get install -y tesseract-ocr\n",
    "    !apt-get install -y libtesseract-dev\n",
    "    \n",
    "    # Clone CRAFT text detection repository\n",
    "    !git clone https://github.com/clovaai/CRAFT-pytorch.git\n",
    "    \n",
    "    print(\"Setup completed!\")\n",
    "else:\n",
    "    print(\"Not running in Colab. Please install required packages manually if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6730ac8",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Let's import all the necessary libraries for our OCR project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cae6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageFilter\n",
    "import cv2\n",
    "\n",
    "# Import PyTorch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import Hugging Face libraries\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "\n",
    "# Import Tesseract for comparison\n",
    "import pytesseract\n",
    "\n",
    "# For visualization\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9af68",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "\n",
    "We'll use multiple sources to create our dataset:\n",
    "1. Khmer dictionary: https://huggingface.co/datasets/seanghay/khmer-dictionary-44k\n",
    "2. Khmer fonts info previews: https://huggingface.co/datasets/seanghay/khmerfonts-info-previews\n",
    "3. Khmer Hanuman dataset: https://huggingface.co/datasets/seanghay/khmer-hanuman-100k\n",
    "\n",
    "Let's start by downloading and exploring these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories to store datasets\n",
    "os.makedirs('dataset', exist_ok=True)\n",
    "os.makedirs('dataset/khmer_dictionary', exist_ok=True)\n",
    "os.makedirs('dataset/khmer_fonts_previews', exist_ok=True)\n",
    "os.makedirs('dataset/khmer_hanuman', exist_ok=True)\n",
    "\n",
    "# Download datasets from Hugging Face\n",
    "print(\"Downloading Khmer Dictionary Dataset...\")\n",
    "khmer_dictionary = load_dataset(\"seanghay/khmer-dictionary-44k\")\n",
    "print(f\"Dataset loaded with {len(khmer_dictionary['train'])} entries\")\n",
    "\n",
    "print(\"\\nDownloading Khmer Fonts Info Previews Dataset...\")\n",
    "khmer_fonts_previews = load_dataset(\"seanghay/khmerfonts-info-previews\")\n",
    "print(f\"Dataset loaded with {len(khmer_fonts_previews['train'])} entries\")\n",
    "\n",
    "print(\"\\nDownloading Khmer Hanuman Dataset...\")\n",
    "khmer_hanuman = load_dataset(\"seanghay/khmer-hanuman-100k\")\n",
    "print(f\"Dataset loaded with {len(khmer_hanuman['train'])} entries\")\n",
    "\n",
    "# Display sample entries from each dataset\n",
    "print(\"\\n--- Sample from Khmer Dictionary ---\")\n",
    "print(khmer_dictionary['train'][0])\n",
    "\n",
    "print(\"\\n--- Sample from Khmer Fonts Previews ---\")\n",
    "print(khmer_fonts_previews['train'][0])\n",
    "\n",
    "print(\"\\n--- Sample from Khmer Hanuman ---\")\n",
    "print(khmer_hanuman['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd038af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from datasets and save to CSV\n",
    "def extract_text_to_csv(dataset, output_file, text_key='text'):\n",
    "    \"\"\"\n",
    "    Extract text from a dataset and save to CSV\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset object\n",
    "        output_file: Path to save the CSV file\n",
    "        text_key: Key to extract text from the dataset\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for item in tqdm(dataset, desc=\"Extracting text\"):\n",
    "        if text_key in item:\n",
    "            texts.append(item[text_key])\n",
    "    \n",
    "    df = pd.DataFrame({'text': texts})\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved {len(texts)} text samples to {output_file}\")\n",
    "    return df\n",
    "\n",
    "# Extract texts from datasets\n",
    "khmer_dict_df = extract_text_to_csv(\n",
    "    khmer_dictionary['train'], \n",
    "    'dataset/khmer_dictionary/text.csv',\n",
    "    text_key='word'\n",
    ")\n",
    "\n",
    "# For khmer_hanuman, we'll use the 'text' field\n",
    "khmer_hanuman_df = extract_text_to_csv(\n",
    "    khmer_hanuman['train'], \n",
    "    'dataset/khmer_hanuman/text.csv'\n",
    ")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample from Khmer Dictionary DataFrame:\")\n",
    "print(khmer_dict_df.head())\n",
    "\n",
    "print(\"\\nSample from Khmer Hanuman DataFrame:\")\n",
    "print(khmer_hanuman_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19111d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Khmer fonts info previews dataset\n",
    "def process_font_info(dataset, max_entries=None):\n",
    "    \"\"\"\n",
    "    Process font info dataset to extract font information and previews\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset object\n",
    "max_entries: Maximum number of entries to process (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of font info and DataFrame with font data\n",
    "    \"\"\"\n",
    "    font_info = {}\n",
    "    font_data = []\n",
    "    \n",
    "# Limit the number of entries if specified\n",
    "    dataset_items = dataset[:max_entries] if max_entries is not None else dataset\n",
    "    \n",
    "    for item in tqdm(dataset_items, desc=\"Processing font info\"):\n",
    "        font_name = item.get('name', 'unknown')\n",
    "        font_url = item.get('url', '')\n",
    "        preview_text = item.get('previewText', '')\n",
    "        \n",
    "        font_info[font_name] = {\n",
    "            'url': font_url,\n",
    "            'preview_text': preview_text\n",
    "        }\n",
    "        \n",
    "        font_data.append({\n",
    "            'name': font_name,\n",
    "            'url': font_url,\n",
    "            'preview_text': preview_text\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    font_df = pd.DataFrame(font_data)\n",
    "    font_df.to_csv('dataset/khmer_fonts_previews/fonts_info.csv', index=False)\n",
    "    \n",
    "    print(f\"Processed {len(font_info)} Khmer fonts\")\n",
    "    return font_info, font_df\n",
    "\n",
    "# Process font info - limit to 10000 entries\n",
    "khmer_fonts_info, khmer_fonts_df = process_font_info(khmer_fonts_previews['train'], max_entries=10000)\n",
    "\n",
    "# Display sample font info\n",
    "print(\"\\nSample from Khmer Fonts DataFrame:\")\n",
    "print(khmer_fonts_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6315f8",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning and Preprocessing\n",
    "\n",
    "Before generating synthetic data, we need to clean and preprocess the text data for both Khmer and English languages. This includes:\n",
    "1. Removing unwanted characters\n",
    "2. Normalizing Unicode for Khmer text\n",
    "3. Preparing text for synthetic image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning functions\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_text(text, is_khmer=True):\n",
    "    \"\"\"\n",
    "    Clean text data by removing unwanted characters and normalizing\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        is_khmer: Boolean indicating if text is Khmer (True) or English (False)\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize Unicode form\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    if is_khmer:\n",
    "        # Keep only Khmer characters, numbers, and basic punctuation\n",
    "        text = re.sub(r'[^\\u1780-\\u17FF\\u19E0-\\u19FF\\s0-9.,!?។៖]', '', text)\n",
    "    else:\n",
    "        # Keep only Latin characters, numbers, and basic punctuation for English\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to prepare dataset for text generation\n",
    "def prepare_text_dataset(text_df, is_khmer=True, min_length=5, max_length=100):\n",
    "    \"\"\"\n",
    "    Prepare text dataset for image generation\n",
    "    \n",
    "    Args:\n",
    "        text_df: DataFrame containing text data\n",
    "        is_khmer: Boolean indicating if text is Khmer (True) or English (False)\n",
    "        min_length: Minimum text length to keep\n",
    "        max_length: Maximum text length to keep\n",
    "    \n",
    "    Returns:\n",
    "        List of cleaned texts\n",
    "    \"\"\"\n",
    "    cleaned_texts = []\n",
    "    \n",
    "    for _, row in tqdm(text_df.iterrows(), total=len(text_df), desc=\"Cleaning texts\"):\n",
    "        text = row['text']\n",
    "        \n",
    "        # Clean text\n",
    "        cleaned = clean_text(text, is_khmer=is_khmer)\n",
    "        \n",
    "        # Filter by length\n",
    "        if min_length <= len(cleaned) <= max_length:\n",
    "            cleaned_texts.append(cleaned)\n",
    "    \n",
    "    print(f\"Prepared {len(cleaned_texts)} text samples for image generation\")\n",
    "    return cleaned_texts\n",
    "\n",
    "# Clean and prepare Khmer text\n",
    "khmer_texts = prepare_text_dataset(khmer_dict_df, is_khmer=True)\n",
    "\n",
    "# Display some cleaned Khmer texts\n",
    "print(\"\\nSample cleaned Khmer texts:\")\n",
    "for i in range(min(5, len(khmer_texts))):\n",
    "    print(f\"{i+1}. {khmer_texts[i]}\")\n",
    "\n",
    "# Add some English text for bilingual capabilities\n",
    "english_texts = [\n",
    "    \"Hello world\",\n",
    "    \"Welcome to Khmer OCR\",\n",
    "    \"Artificial intelligence\",\n",
    "    \"Computer vision and deep learning\",\n",
    "    \"Optical character recognition\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Machine learning algorithms process data\",\n",
    "    \"Python is a programming language\",\n",
    "    \"Natural language processing with transformers\",\n",
    "    \"This is an OCR system for Khmer and English text\"\n",
    "]\n",
    "\n",
    "print(\"\\nSample English texts:\")\n",
    "for i, text in enumerate(english_texts[:5]):\n",
    "    print(f\"{i+1}. {text}\")\n",
    "\n",
    "# Combine into a single dataset\n",
    "all_texts = khmer_texts + english_texts\n",
    "print(f\"\\nTotal texts after combining: {len(all_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cea15d",
   "metadata": {},
   "source": [
    "## 4. Text-to-Image Synthesis for Synthetic Data\n",
    "\n",
    "We'll generate synthetic images from the cleaned text data using various fonts for both Khmer and English. This will provide a large training dataset for our OCR models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562835e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories to store synthetic data\n",
    "os.makedirs('dataset/synthetic_data', exist_ok=True)\n",
    "os.makedirs('dataset/synthetic_data/images', exist_ok=True)\n",
    "os.makedirs('dataset/fonts', exist_ok=True)\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Downloading Khmer and English fonts...\")\n",
    "    \n",
    "    # Create a subdirectory for organizing fonts\n",
    "    os.makedirs('dataset/fonts/khmer', exist_ok=True)\n",
    "    os.makedirs('dataset/fonts/english', exist_ok=True)\n",
    "    \n",
    "    # Download Khmer fonts from GitHub repositories\n",
    "    print(\"Downloading Khmer fonts from GitHub repositories...\")\n",
    "    !wget -q -O dataset/fonts/khmer/Khmer.ttf https://github.com/danhhong/khmer-fonts/raw/master/fonts/Khmer/Khmer.ttf\n",
    "    !wget -q -O dataset/fonts/khmer/KhmerOSbattambang.ttf https://github.com/danhhong/khmer-fonts/raw/master/fonts/KhmerOS/KhmerOSbattambang.ttf\n",
    "    !wget -q -O dataset/fonts/khmer/KhmerOSbokor.ttf https://github.com/danhhong/khmer-fonts/raw/master/fonts/KhmerOS/KhmerOSbokor.ttf\n",
    "    !wget -q -O dataset/fonts/khmer/KhmerOSfasthand.ttf https://github.com/danhhong/khmer-fonts/raw/master/fonts/KhmerOS/KhmerOSfasthand.ttf\n",
    "    !wget -q -O dataset/fonts/khmer/KhmerOSfreehand.ttf https://github.com/danhhong/khmer-fonts/raw/master/fonts/KhmerOS/KhmerOSfreehand.ttf\n",
    "    !wget -q -O dataset/fonts/khmer/KhmerOSmuol.ttf https://github.com/danhhong/khmer-fonts/raw/master/fonts/KhmerOS/KhmerOSmuol.ttf\n",
    "    \n",
    "    # Fix for KhmerOSsiemreap - using raw content URL\n",
    "    !wget -q -O dataset/fonts/khmer/KhmerOSsiemreap.ttf https://github.com/danhhong/Siemreap/raw/master/Siemreap-Regular.ttf\n",
    "    \n",
    "    # Download popular Khmer fonts from Google Fonts\n",
    "    print(\"Downloading Khmer fonts from Google Fonts...\")\n",
    "    # Moul font\n",
    "    !wget -q -O dataset/fonts/khmer/Moul.ttf https://github.com/google/fonts/raw/main/ofl/moul/Moul.ttf\n",
    "    # Koulen font\n",
    "    !wget -q -O dataset/fonts/khmer/Koulen.ttf https://github.com/google/fonts/raw/main/ofl/koulen/Koulen.ttf\n",
    "    # Bayon font\n",
    "    !wget -q -O dataset/fonts/khmer/Bayon.ttf https://github.com/google/fonts/raw/main/ofl/bayon/Bayon.ttf\n",
    "    # Content font\n",
    "    !wget -q -O dataset/fonts/khmer/Content.ttf https://github.com/google/fonts/raw/main/ofl/content/Content-Regular.ttf\n",
    "    # Dangrek font\n",
    "    !wget -q -O dataset/fonts/khmer/Dangrek.ttf https://github.com/google/fonts/raw/main/ofl/dangrek/Dangrek.ttf\n",
    "    # Bokor font\n",
    "    !wget -q -O dataset/fonts/khmer/Bokor.ttf https://github.com/google/fonts/raw/main/ofl/bokor/Bokor.ttf\n",
    "    # Suwannaphum font\n",
    "    !wget -q -O dataset/fonts/khmer/Suwannaphum.ttf https://github.com/google/fonts/raw/main/ofl/suwannaphum/Suwannaphum-Regular.ttf\n",
    "    # Battambang font\n",
    "    !wget -q -O dataset/fonts/khmer/Battambang.ttf https://github.com/google/fonts/raw/main/ofl/battambang/Battambang-Regular.ttf\n",
    "    \n",
    "    # Additional fonts from khmerfonts.info (via GitHub mirrors or other reliable sources)\n",
    "    print(\"Downloading additional Khmer fonts...\")\n",
    "    # Kh Battambang font\n",
    "    !wget -q -O dataset/fonts/khmer/Kh_Battambang.ttf https://github.com/danhhong/khmer-fonts/raw/master/fonts/Battambang/Battambang-Regular.ttf\n",
    "    # Khmer M1 font\n",
    "    !wget -q -O dataset/fonts/khmer/Khmer_M1.ttf https://github.com/danhhong/khmer-fonts/raw/master/fonts/Khmer_M1/Khmer_M1.ttf\n",
    "    # Khmer M2 font\n",
    "    !wget -q -O dataset/fonts/khmer/Khmer_M2.ttf https://github.com/danhhong/khmer-fonts/raw/master/fonts/Khmer_M2/Khmer_M2.ttf\n",
    "    \n",
    "    # Add some English fonts\n",
    "    print(\"Downloading English fonts...\")\n",
    "    !wget -q -O dataset/fonts/english/Arial.ttf https://github.com/matomo-org/travis-scripts/raw/master/fonts/Arial.ttf\n",
    "    !wget -q -O dataset/fonts/english/Times_New_Roman.ttf https://github.com/matomo-org/travis-scripts/raw/master/fonts/Times_New_Roman.ttf\n",
    "    !wget -q -O dataset/fonts/english/Courier_New.ttf https://github.com/matomo-org/travis-scripts/raw/master/fonts/Courier_New.ttf\n",
    "\n",
    "    print(\"Downloaded fonts successfully!\")\n",
    "else:\n",
    "    print(\"Please download Khmer and English fonts manually and place them in the dataset/fonts directory\")\n",
    "    print(\"You can get Khmer fonts from:\")\n",
    "    print(\"1. Google Fonts: https://fonts.google.com/?subset=khmer\")\n",
    "    print(\"2. KhmerFonts.info: https://www.khmerfonts.info/\")\n",
    "    print(\"3. GitHub repository: https://github.com/danhhong/khmer-fonts\")\n",
    "\n",
    "# Combine fonts from both directories for easier access\n",
    "fonts = glob.glob('dataset/fonts/**/*.ttf', recursive=True)\n",
    "if not fonts:\n",
    "    fonts = glob.glob('dataset/fonts/*.ttf')  # Fallback to flat structure if no fonts in subdirectories\n",
    "\n",
    "print(f\"Available fonts ({len(fonts)}):\")\n",
    "for font in fonts:\n",
    "    print(f\"- {os.path.basename(font)}\")\n",
    "\n",
    "# Function to check if a font supports Khmer characters\n",
    "def font_supports_khmer(font_path):\n",
    "    \"\"\"Check if a font supports Khmer characters\"\"\"\n",
    "    try:\n",
    "        # Try to create a test image with Khmer text\n",
    "        font = ImageFont.truetype(font_path, 24)\n",
    "        img = Image.new('RGB', (200, 50), color=(255, 255, 255))\n",
    "        d = ImageDraw.Draw(img)\n",
    "        d.text((10, 10), \"កខគឃង\", font=font, fill=(0, 0, 0))\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Check which fonts support Khmer\n",
    "print(\"\\nFonts supporting Khmer:\")\n",
    "khmer_fonts = []\n",
    "english_fonts = []\n",
    "\n",
    "for font_path in fonts:\n",
    "    if font_supports_khmer(font_path):\n",
    "        print(f\"- {os.path.basename(font_path)} (Supports Khmer)\")\n",
    "        khmer_fonts.append(font_path)\n",
    "    else:\n",
    "        print(f\"- {os.path.basename(font_path)} (English only)\")\n",
    "        english_fonts.append(font_path)\n",
    "\n",
    "print(f\"\\nFound {len(khmer_fonts)} Khmer fonts and {len(english_fonts)} English-only fonts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55033565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a preview of Khmer fonts\n",
    "def display_font_preview(fonts, sample_text=\"ជំរាបសួរ សុខសប្បាយ\", size=(800, 100), font_size=32):\n",
    "    \"\"\"\n",
    "    Display a preview of fonts with the given sample text\n",
    "    \n",
    "    Args:\n",
    "        fonts: List of font paths to preview\n",
    "        sample_text: Text to display in the preview\n",
    "        size: Size of the preview image (width, height)\n",
    "        font_size: Font size to use\n",
    "    \"\"\"\n",
    "    num_fonts = len(fonts)\n",
    "    if num_fonts == 0:\n",
    "        print(\"No fonts to preview\")\n",
    "        return\n",
    "    \n",
    "    # Create a figure to display all font previews\n",
    "    fig_height = max(8, num_fonts * 1.5)\n",
    "    plt.figure(figsize=(15, fig_height))\n",
    "    \n",
    "    for i, font_path in enumerate(fonts):\n",
    "        try:\n",
    "            # Create image for font preview\n",
    "            img = Image.new('RGB', size, color=(255, 255, 255))\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            font = ImageFont.truetype(font_path, font_size)\n",
    "            \n",
    "            # Draw font info\n",
    "            font_name = os.path.basename(font_path)\n",
    "            \n",
    "            # Calculate position\n",
    "            text_width, text_height = draw.textsize(sample_text, font=font) if hasattr(draw, 'textsize') else font.getsize(sample_text)\n",
    "            x = (size[0] - text_width) // 2\n",
    "            y = (size[1] - text_height) // 2\n",
    "            \n",
    "            # Draw text\n",
    "            draw.text((x, y), sample_text, font=font, fill=(0, 0, 0))\n",
    "            \n",
    "            # Display in the figure\n",
    "            plt.subplot(num_fonts, 1, i+1)\n",
    "            plt.imshow(np.array(img))\n",
    "            plt.title(f\"{font_name}\")\n",
    "            plt.axis('off')\n",
    "        except Exception as e:\n",
    "            print(f\"Error previewing font {os.path.basename(font_path)}: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display preview of Khmer fonts\n",
    "if khmer_fonts:\n",
    "    print(f\"Previewing {min(10, len(khmer_fonts))} Khmer fonts:\")\n",
    "    display_font_preview(khmer_fonts[:10])\n",
    "else:\n",
    "    print(\"No Khmer fonts available for preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a synthetic text image\n",
    "def generate_text_image(text, font_path, width=800, height=200, \n",
    "                        bg_color=(255, 255, 255), text_color=(0, 0, 0),\n",
    "                        font_size=36, padding=20, is_khmer=True):\n",
    "    \"\"\"\n",
    "    Generate a synthetic text image\n",
    "    \n",
    "    Args:\n",
    "        text: Text to render\n",
    "        font_path: Path to the font file\n",
    "        width, height: Image dimensions\n",
    "        bg_color: Background color\n",
    "        text_color: Text color\n",
    "        font_size: Font size\n",
    "        padding: Padding around text\n",
    "        is_khmer: Boolean indicating if text is Khmer\n",
    "    \n",
    "    Returns:\n",
    "        PIL Image with rendered text\n",
    "    \"\"\"\n",
    "    # Create blank image\n",
    "    img = Image.new('RGB', (width, height), color=bg_color)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Load font\n",
    "    try:\n",
    "        font = ImageFont.truetype(font_path, font_size)\n",
    "    except:\n",
    "        # Fallback to default font\n",
    "        print(f\"Error loading font {font_path}, using default\")\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Calculate text size and position\n",
    "    text_width, text_height = draw.textsize(text, font=font) if hasattr(draw, 'textsize') else font.getsize(text)\n",
    "    \n",
    "    # Adjust size if text is too large\n",
    "    if text_width > width - 2*padding:\n",
    "        # Calculate new font size to fit width\n",
    "        new_font_size = int(font_size * (width - 2*padding) / text_width)\n",
    "        font = ImageFont.truetype(font_path, new_font_size)\n",
    "        text_width, text_height = draw.textsize(text, font=font) if hasattr(draw, 'textsize') else font.getsize(text)\n",
    "    \n",
    "    # Center text\n",
    "    x = (width - text_width) // 2\n",
    "    y = (height - text_height) // 2\n",
    "    \n",
    "    # Draw text\n",
    "    draw.text((x, y), text, font=font, fill=text_color)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Function to generate a dataset of synthetic images\n",
    "def generate_synthetic_dataset(texts, fonts, output_dir, num_samples=1000, \n",
    "                               is_khmer=True, min_font_size=24, max_font_size=48):\n",
    "    \"\"\"\n",
    "    Generate a dataset of synthetic text images\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to use\n",
    "        fonts: List of font paths\n",
    "        output_dir: Output directory\n",
    "        num_samples: Number of samples to generate\n",
    "        is_khmer: Boolean indicating if text is Khmer\n",
    "        min_font_size: Minimum font size\n",
    "        max_font_size: Maximum font size\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with image paths and labels\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    data = []\n",
    "    bg_colors = [(255, 255, 255), (240, 240, 240), (250, 250, 250), (245, 245, 245)]\n",
    "    text_colors = [(0, 0, 0), (50, 50, 50), (70, 70, 70), (30, 30, 30)]\n",
    "    \n",
    "    for i in tqdm(range(num_samples), desc=\"Generating synthetic images\"):\n",
    "        # Randomly select a text and font\n",
    "        text = random.choice(texts)\n",
    "        font = random.choice(fonts)\n",
    "        \n",
    "        # Random parameters\n",
    "        font_size = random.randint(min_font_size, max_font_size)\n",
    "        bg_color = random.choice(bg_colors)\n",
    "        text_color = random.choice(text_colors)\n",
    "        width = random.randint(400, 800)\n",
    "        height = random.randint(100, 200)\n",
    "        \n",
    "        # Generate image\n",
    "        img = generate_text_image(\n",
    "            text=text,\n",
    "            font_path=font,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            bg_color=bg_color,\n",
    "            text_color=text_color,\n",
    "            font_size=font_size,\n",
    "            is_khmer=is_khmer\n",
    "        )\n",
    "        \n",
    "        # Save image\n",
    "        img_path = os.path.join(output_dir, f\"syn_{i:06d}.png\")\n",
    "        img.save(img_path)\n",
    "        \n",
    "        data.append({\n",
    "            'image_path': img_path,\n",
    "            'text': text,\n",
    "            'is_khmer': is_khmer,\n",
    "            'font': os.path.basename(font)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    csv_path = os.path.join(os.path.dirname(output_dir), f\"synthetic_data_{is_khmer}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"Generated {len(data)} synthetic images\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set smaller numbers for demonstration purposes\n",
    "# In a real project, you'd want to generate thousands of samples\n",
    "num_khmer_samples = 100  # Increase this for real training\n",
    "num_english_samples = 50  # Increase this for real training\n",
    "\n",
    "# Generate Khmer synthetic data\n",
    "khmer_synthetic_dir = 'dataset/synthetic_data/khmer'\n",
    "os.makedirs(khmer_synthetic_dir, exist_ok=True)\n",
    "\n",
    "# Limit texts for demonstration\n",
    "khmer_texts_sample = khmer_texts[:500] if len(khmer_texts) > 500 else khmer_texts\n",
    "\n",
    "# Generate Khmer synthetic data\n",
    "if khmer_fonts:\n",
    "    khmer_synthetic_df = generate_synthetic_dataset(\n",
    "        texts=khmer_texts_sample,\n",
    "        fonts=khmer_fonts,\n",
    "        output_dir=khmer_synthetic_dir,\n",
    "        num_samples=num_khmer_samples,\n",
    "        is_khmer=True\n",
    "    )\n",
    "\n",
    "# Generate English synthetic data\n",
    "english_synthetic_dir = 'dataset/synthetic_data/english'\n",
    "os.makedirs(english_synthetic_dir, exist_ok=True)\n",
    "\n",
    "# Generate English synthetic data\n",
    "english_synthetic_df = generate_synthetic_dataset(\n",
    "    texts=english_texts,\n",
    "    fonts=english_fonts if english_fonts else khmer_fonts,  # Use Khmer fonts as fallback\n",
    "    output_dir=english_synthetic_dir,\n",
    "    num_samples=num_english_samples,\n",
    "    is_khmer=False\n",
    ")\n",
    "\n",
    "# Display some examples\n",
    "def display_samples(df, num_samples=3):\n",
    "    \"\"\"Display sample images and their labels\"\"\"\n",
    "    plt.figure(figsize=(15, 5*num_samples))\n",
    "    \n",
    "    for i in range(min(num_samples, len(df))):\n",
    "        img_path = df['image_path'].iloc[i]\n",
    "        text = df['text'].iloc[i]\n",
    "        is_khmer = df['is_khmer'].iloc[i]\n",
    "        \n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        plt.subplot(num_samples, 1, i+1)\n",
    "        plt.imshow(np.array(img))\n",
    "        plt.title(f\"{'Khmer' if is_khmer else 'English'}: {text}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display samples from both datasets\n",
    "print(\"Khmer Synthetic Data Samples:\")\n",
    "if 'khmer_synthetic_df' in locals():\n",
    "    display_samples(khmer_synthetic_df)\n",
    "else:\n",
    "    print(\"No Khmer synthetic data generated\")\n",
    "\n",
    "print(\"English Synthetic Data Samples:\")\n",
    "display_samples(english_synthetic_df)\n",
    "\n",
    "# Combine datasets\n",
    "synthetic_dfs = []\n",
    "if 'khmer_synthetic_df' in locals():\n",
    "    synthetic_dfs.append(khmer_synthetic_df)\n",
    "synthetic_dfs.append(english_synthetic_df)\n",
    "\n",
    "synthetic_df = pd.concat(synthetic_dfs, ignore_index=True)\n",
    "synthetic_df.to_csv('dataset/synthetic_data/all_synthetic_data.csv', index=False)\n",
    "\n",
    "print(f\"Total synthetic dataset size: {len(synthetic_df)} images\")\n",
    "print(f\"Khmer: {len(synthetic_df[synthetic_df['is_khmer']])} images\")\n",
    "print(f\"English: {len(synthetic_df[~synthetic_df['is_khmer']])} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6b5ab8",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation Techniques\n",
    "\n",
    "To make our OCR system robust to real-world conditions, we'll apply various data augmentation techniques to our synthetic images, including:\n",
    "1. Rotation and skew\n",
    "2. Adding noise and blur\n",
    "3. Changing contrast and brightness\n",
    "4. Adding complex backgrounds\n",
    "5. Simulating shadows and lighting effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6198db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for augmented data\n",
    "os.makedirs('dataset/augmented_data', exist_ok=True)\n",
    "\n",
    "# Define augmentation functions\n",
    "def rotate_image(image, max_angle=10):\n",
    "    \"\"\"Rotate image by a random angle\"\"\"\n",
    "    angle = random.uniform(-max_angle, max_angle)\n",
    "    return image.rotate(angle, resample=Image.BILINEAR, expand=False)\n",
    "\n",
    "def add_noise(image, noise_level=10):\n",
    "    \"\"\"Add random noise to image\"\"\"\n",
    "    img_array = np.array(image)\n",
    "    noise = np.random.randint(-noise_level, noise_level, img_array.shape)\n",
    "    noisy_img_array = np.clip(img_array + noise, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(noisy_img_array)\n",
    "\n",
    "def adjust_contrast_brightness(image, contrast_factor_range=(0.8, 1.2), brightness_factor_range=(0.8, 1.2)):\n",
    "    \"\"\"Adjust image contrast and brightness\"\"\"\n",
    "    contrast_factor = random.uniform(*contrast_factor_range)\n",
    "    brightness_factor = random.uniform(*brightness_factor_range)\n",
    "    \n",
    "    # Convert to numpy for easier manipulation\n",
    "    img_array = np.array(image).astype(np.float32)\n",
    "    \n",
    "    # Adjust contrast\n",
    "    img_array = (img_array - 128) * contrast_factor + 128\n",
    "    \n",
    "    # Adjust brightness\n",
    "    img_array = img_array * brightness_factor\n",
    "    \n",
    "    # Clip values and convert back to uint8\n",
    "    img_array = np.clip(img_array, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return Image.fromarray(img_array)\n",
    "\n",
    "def add_blur(image, radius=1.5):\n",
    "    \"\"\"Apply Gaussian blur to image\"\"\"\n",
    "    return image.filter(ImageFilter.GaussianBlur(radius=radius))\n",
    "\n",
    "def apply_perspective_transform(image, max_distortion=30):\n",
    "    \"\"\"Apply random perspective transform to image\"\"\"\n",
    "    width, height = image.size\n",
    "    \n",
    "    # Define source points\n",
    "    src_points = np.float32([\n",
    "        [0, 0],\n",
    "        [width - 1, 0],\n",
    "        [0, height - 1],\n",
    "        [width - 1, height - 1]\n",
    "    ])\n",
    "    \n",
    "    # Define destination points with random distortion\n",
    "    dst_points = src_points + np.random.uniform(-max_distortion, max_distortion, src_points.shape)\n",
    "    \n",
    "    # Ensure points stay within image bounds\n",
    "    dst_points[:, 0] = np.clip(dst_points[:, 0], 0, width - 1)\n",
    "    dst_points[:, 1] = np.clip(dst_points[:, 1], 0, height - 1)\n",
    "    \n",
    "    # Convert to opencv format\n",
    "    src_points = src_points.astype(np.float32)\n",
    "    dst_points = dst_points.astype(np.float32)\n",
    "    \n",
    "    # Calculate perspective transform matrix\n",
    "    transform_matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "    \n",
    "    # Apply transform\n",
    "    img_array = np.array(image)\n",
    "    transformed_img = cv2.warpPerspective(img_array, transform_matrix, (width, height))\n",
    "    \n",
    "    return Image.fromarray(transformed_img)\n",
    "\n",
    "# Function to apply random augmentations\n",
    "def apply_random_augmentations(image):\n",
    "    \"\"\"Apply random augmentations to an image\"\"\"\n",
    "    # List of augmentation functions with their probabilities\n",
    "    augmentations = [\n",
    "        (rotate_image, 0.7),\n",
    "        (add_noise, 0.5),\n",
    "        (adjust_contrast_brightness, 0.6),\n",
    "        (add_blur, 0.4),\n",
    "        (apply_perspective_transform, 0.3)\n",
    "    ]\n",
    "    \n",
    "    # Apply augmentations based on their probabilities\n",
    "    for aug_func, prob in augmentations:\n",
    "        if random.random() < prob:\n",
    "            image = aug_func(image)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Function to augment a dataset\n",
    "def augment_dataset(df, output_dir, augmentations_per_image=2):\n",
    "    \"\"\"\n",
    "    Augment images in a dataset\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with image paths and labels\n",
    "        output_dir: Output directory for augmented images\n",
    "        augmentations_per_image: Number of augmentations to generate per image\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with augmented image paths and labels\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    augmented_data = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Augmenting images\"):\n",
    "        img_path = row['image_path']\n",
    "        text = row['text']\n",
    "        is_khmer = row['is_khmer']\n",
    "        \n",
    "        # Load original image\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "        except:\n",
    "            print(f\"Error loading image {img_path}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Generate augmentations\n",
    "        for i in range(augmentations_per_image):\n",
    "            # Apply random augmentations\n",
    "            aug_img = apply_random_augmentations(image)\n",
    "            \n",
    "            # Save augmented image\n",
    "            aug_img_path = os.path.join(output_dir, f\"aug_{idx}_{i}.png\")\n",
    "            aug_img.save(aug_img_path)\n",
    "            \n",
    "            # Add to dataset\n",
    "            augmented_data.append({\n",
    "                'image_path': aug_img_path,\n",
    "                'text': text,\n",
    "                'is_khmer': is_khmer,\n",
    "                'original_image': img_path\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    aug_df = pd.DataFrame(augmented_data)\n",
    "    csv_path = os.path.join(os.path.dirname(output_dir), f\"augmented_data.csv\")\n",
    "    aug_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"Generated {len(augmented_data)} augmented images\")\n",
    "    return aug_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214acd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentations to a subset of the synthetic data\n",
    "sample_size = min(20, len(synthetic_df))  # Limit for demonstration\n",
    "synthetic_sample_df = synthetic_df.sample(sample_size)\n",
    "\n",
    "# Augment the sample\n",
    "augmented_df = augment_dataset(\n",
    "    synthetic_sample_df,\n",
    "    'dataset/augmented_data/images',\n",
    "    augmentations_per_image=2  # Generate 2 augmentations per image\n",
    ")\n",
    "\n",
    "# Display some original and augmented images side by side\n",
    "def display_original_and_augmented(aug_df, num_samples=3):\n",
    "    \"\"\"Display original and augmented images side by side\"\"\"\n",
    "    plt.figure(figsize=(15, 5*num_samples))\n",
    "    \n",
    "    for i in range(min(num_samples, len(aug_df))):\n",
    "        aug_img_path = aug_df['image_path'].iloc[i]\n",
    "        orig_img_path = aug_df['original_image'].iloc[i]\n",
    "        text = aug_df['text'].iloc[i]\n",
    "        \n",
    "        aug_img = Image.open(aug_img_path)\n",
    "        orig_img = Image.open(orig_img_path)\n",
    "        \n",
    "        plt.subplot(num_samples, 2, 2*i+1)\n",
    "        plt.imshow(np.array(orig_img))\n",
    "        plt.title(f\"Original: {text}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(num_samples, 2, 2*i+2)\n",
    "        plt.imshow(np.array(aug_img))\n",
    "        plt.title(f\"Augmented: {text}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display samples\n",
    "print(\"Original vs. Augmented Images:\")\n",
    "display_original_and_augmented(augmented_df)\n",
    "\n",
    "# Combine synthetic and augmented data for training\n",
    "combined_df = pd.concat([synthetic_df, augmented_df], ignore_index=True)\n",
    "combined_df.to_csv('dataset/combined_data.csv', index=False)\n",
    "\n",
    "print(f\"Combined dataset size: {len(combined_df)} images\")\n",
    "print(f\"Synthetic: {len(synthetic_df)} images\")\n",
    "print(f\"Augmented: {len(augmented_df)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f7b563",
   "metadata": {},
   "source": [
    "## 6. Text Detection with CRAFT\n",
    "\n",
    "We'll use the CRAFT (Character-Region Awareness for Text Detection) model for text detection. CRAFT is a state-of-the-art text detection model that can detect text regions in images with high accuracy.\n",
    "\n",
    "Reference: [CRAFT-pytorch](https://github.com/clovaai/CRAFT-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CRAFT pre-trained model\n",
    "if IN_COLAB:\n",
    "    # Make sure CRAFT repository is cloned\n",
    "    if not os.path.exists('CRAFT-pytorch'):\n",
    "        !git clone https://github.com/clovaai/CRAFT-pytorch.git\n",
    "        \n",
    "    # Download pre-trained model\n",
    "    !wget -q -O CRAFT-pytorch/craft_mlt_25k.pth https://drive.google.com/uc?id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ\n",
    "    \n",
    "    # Add CRAFT directory to path for imports\n",
    "    import sys\n",
    "    sys.path.append('CRAFT-pytorch')\n",
    "    \n",
    "    print(\"CRAFT model downloaded successfully!\")\n",
    "else:\n",
    "    print(\"Please download CRAFT model manually:\")\n",
    "    print(\"1. Clone https://github.com/clovaai/CRAFT-pytorch.git\")\n",
    "    print(\"2. Download pre-trained model from https://drive.google.com/file/d/1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ\")\n",
    "    print(\"3. Save it as CRAFT-pytorch/craft_mlt_25k.pth\")\n",
    "\n",
    "# Import CRAFT modules\n",
    "try:\n",
    "    from CRAFT-pytorch.craft import CRAFT\n",
    "    from CRAFT-pytorch.test import test_net\n",
    "    from CRAFT-pytorch.imgproc import resize_aspect_ratio, normalizeMeanVariance\n",
    "    from CRAFT-pytorch.craft_utils import getDetBoxes, adjustResultCoordinates\n",
    "    \n",
    "    # Fix imports if needed by creating wrapper functions\n",
    "    CRAFT_AVAILABLE = True\n",
    "    print(\"CRAFT modules imported successfully!\")\n",
    "except:\n",
    "    CRAFT_AVAILABLE = False\n",
    "    print(\"Could not import CRAFT modules. Creating placeholder functions for demonstration.\")\n",
    "    \n",
    "    # Create placeholder classes and functions for demonstration\n",
    "    class CRAFT(nn.Module):\n",
    "        def __init__(self, pretrained=False):\n",
    "            super(CRAFT, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "            self.conv2 = nn.Conv2d(16, 2, kernel_size=3, padding=1)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = self.conv2(x)\n",
    "            return x\n",
    "\n",
    "    def resize_aspect_ratio(img, target_size, interpolation=cv2.INTER_LINEAR):\n",
    "        h, w, _ = img.shape\n",
    "        ratio = target_size / max(h, w)\n",
    "        return cv2.resize(img, (0, 0), fx=ratio, fy=ratio, interpolation=interpolation)\n",
    "\n",
    "    def normalizeMeanVariance(img, mean=(0.485, 0.456, 0.406), variance=(0.229, 0.224, 0.225)):\n",
    "        img = img.copy().astype(np.float32)\n",
    "        img /= 255.0\n",
    "        img -= mean\n",
    "        img /= variance\n",
    "        return img\n",
    "\n",
    "    def getDetBoxes(textmap, linkmap, text_threshold=0.7, link_threshold=0.4, low_text=0.4):\n",
    "        boxes = []\n",
    "        # Simulate some boxes for demo\n",
    "        h, w = textmap.shape\n",
    "        boxes.append(np.array([[0.1*w, 0.1*h], [0.9*w, 0.1*h], [0.9*w, 0.9*h], [0.1*w, 0.9*h]]))\n",
    "        return boxes, None\n",
    "\n",
    "    def adjustResultCoordinates(boxes, ratio_w, ratio_h):\n",
    "        return [box * [ratio_w, ratio_h] for box in boxes]\n",
    "\n",
    "    def test_net(net, image, text_threshold, link_threshold, low_text, poly):\n",
    "        # Placeholder function\n",
    "        img_resized = resize_aspect_ratio(image, 1280)\n",
    "        ratio_h = image.shape[0] / img_resized.shape[0]\n",
    "        ratio_w = image.shape[1] / img_resized.shape[1]\n",
    "        \n",
    "        # Get fake results\n",
    "        boxes = []\n",
    "        h, w, _ = image.shape\n",
    "        boxes.append(np.array([[0.1*w, 0.1*h], [0.9*w, 0.1*h], [0.9*w, 0.9*h], [0.1*w, 0.9*h]]))\n",
    "        \n",
    "        return boxes, image\n",
    "\n",
    "# CRAFT model setup\n",
    "def load_craft_model():\n",
    "    \"\"\"Load CRAFT model\"\"\"\n",
    "    if not CRAFT_AVAILABLE:\n",
    "        print(\"CRAFT not available, using placeholder model\")\n",
    "        net = CRAFT()\n",
    "        return net\n",
    "    \n",
    "    # Load real CRAFT model\n",
    "    craft_net = CRAFT()\n",
    "    \n",
    "    # Load weights\n",
    "    try:\n",
    "        craft_net.load_state_dict(torch.load('CRAFT-pytorch/craft_mlt_25k.pth', map_location=device))\n",
    "        print(\"CRAFT model loaded successfully!\")\n",
    "    except:\n",
    "        print(\"Could not load CRAFT weights. Using untrained model.\")\n",
    "    \n",
    "    craft_net.eval()\n",
    "    craft_net = craft_net.to(device)\n",
    "    \n",
    "    return craft_net\n",
    "\n",
    "# Load CRAFT model\n",
    "craft_net = load_craft_model()\n",
    "\n",
    "# Text detection function\n",
    "def detect_text(image_path, craft_net=None, text_threshold=0.7, link_threshold=0.4, low_text=0.4, poly=False):\n",
    "    \"\"\"\n",
    "    Detect text regions in an image using CRAFT\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image\n",
    "        craft_net: CRAFT model\n",
    "        text_threshold: Text confidence threshold\n",
    "        link_threshold: Link confidence threshold\n",
    "        low_text: Text low-bound threshold\n",
    "        poly: Use polygon output if True, otherwise use rectangle\n",
    "    \n",
    "    Returns:\n",
    "        boxes: List of detected text boxes\n",
    "        image: Original image\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if craft_net is None:\n",
    "        craft_net = load_craft_model()\n",
    "    \n",
    "    # Detect text regions\n",
    "    boxes, image = test_net(craft_net, image, text_threshold, link_threshold, low_text, poly)\n",
    "    \n",
    "    return boxes, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize detected text regions\n",
    "def visualize_text_detection(image_path, boxes):\n",
    "    \"\"\"\n",
    "    Visualize text detection results\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image\n",
    "        boxes: List of detected text boxes\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create a copy for visualization\n",
    "    viz_image = image.copy()\n",
    "    \n",
    "    # Draw boxes\n",
    "    for box in boxes:\n",
    "        # Convert box to integer points\n",
    "        box = box.astype(np.int32)\n",
    "        \n",
    "        # Draw polygon\n",
    "        cv2.polylines(viz_image, [box], True, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display original and annotated images side by side\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(viz_image)\n",
    "    plt.title(\"Text Detection\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return viz_image\n",
    "\n",
    "# Function to crop detected text regions\n",
    "def crop_text_regions(image_path, boxes, padding=10):\n",
    "    \"\"\"\n",
    "    Crop detected text regions from an image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image\n",
    "        boxes: List of detected text boxes\n",
    "        padding: Padding around text regions\n",
    "    \n",
    "    Returns:\n",
    "        crops: List of cropped text regions\n",
    "        crop_boxes: List of corresponding boxes\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    h, w, _ = image.shape\n",
    "    crops = []\n",
    "    crop_boxes = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Get bounding rectangle\n",
    "        rect = cv2.boundingRect(box.astype(np.int32))\n",
    "        x, y, width, height = rect\n",
    "        \n",
    "        # Add padding\n",
    "        x_min = max(0, x - padding)\n",
    "        y_min = max(0, y - padding)\n",
    "        x_max = min(w, x + width + padding)\n",
    "        y_max = min(h, y + height + padding)\n",
    "        \n",
    "        # Crop region\n",
    "        crop = image[y_min:y_max, x_min:x_max]\n",
    "        crops.append(crop)\n",
    "        crop_boxes.append([x_min, y_min, x_max, y_max])\n",
    "    \n",
    "    return crops, crop_boxes\n",
    "\n",
    "# Test on a few sample images\n",
    "sample_images = []\n",
    "\n",
    "# Use a few synthetic or augmented images\n",
    "if 'synthetic_df' in locals() and len(synthetic_df) > 0:\n",
    "    sample_images.extend(synthetic_df['image_path'].sample(min(3, len(synthetic_df))).tolist())\n",
    "\n",
    "if 'augmented_df' in locals() and len(augmented_df) > 0:\n",
    "    sample_images.extend(augmented_df['image_path'].sample(min(2, len(augmented_df))).tolist())\n",
    "\n",
    "# Test CRAFT text detection on sample images\n",
    "for img_path in sample_images:\n",
    "    print(f\"Processing image: {os.path.basename(img_path)}\")\n",
    "    \n",
    "    # Detect text regions\n",
    "    boxes, image = detect_text(img_path, craft_net)\n",
    "    \n",
    "    # Visualize results\n",
    "    viz_image = visualize_text_detection(img_path, boxes)\n",
    "    \n",
    "    # Crop text regions\n",
    "    crops, crop_boxes = crop_text_regions(img_path, boxes)\n",
    "    \n",
    "    # Display cropped regions\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.suptitle(\"Cropped Text Regions\")\n",
    "    \n",
    "    for i, crop in enumerate(crops[:5]):  # Display up to 5 crops\n",
    "        plt.subplot(1, len(crops[:5]), i+1)\n",
    "        plt.imshow(crop)\n",
    "        plt.title(f\"Region {i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0641a8a",
   "metadata": {},
   "source": [
    "## 7. Text Recognition with TrOCR\n",
    "\n",
    "Now that we have detected text regions, we'll use the TrOCR model to recognize text from these regions. TrOCR is a transformer-based OCR model that achieves state-of-the-art results on various OCR tasks.\n",
    "\n",
    "We'll use the pre-trained TrOCR model from Hugging Face: [microsoft/trocr-large-printed](https://huggingface.co/microsoft/trocr-large-printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TrOCR model and processor\n",
    "def load_trocr_model():\n",
    "    \"\"\"\n",
    "    Load the TrOCR model and processor\n",
    "    \n",
    "    Returns:\n",
    "        processor: TrOCR processor\n",
    "        model: TrOCR model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load processor and model from Hugging Face\n",
    "        processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-printed\")\n",
    "        model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-printed\")\n",
    "        \n",
    "        # Move model to device\n",
    "        model = model.to(device)\n",
    "        \n",
    "        print(\"TrOCR model loaded successfully!\")\n",
    "        return processor, model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading TrOCR model: {e}\")\n",
    "        print(\"Using placeholder for demonstration purposes.\")\n",
    "        \n",
    "        # Return None values to indicate model not loaded\n",
    "        return None, None\n",
    "\n",
    "# Load TrOCR model\n",
    "trocr_processor, trocr_model = load_trocr_model()\n",
    "\n",
    "# Function to recognize text using TrOCR\n",
    "def recognize_text(image, processor=None, model=None):\n",
    "    \"\"\"\n",
    "    Recognize text in an image using TrOCR\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image or numpy array\n",
    "        processor: TrOCR processor\n",
    "        model: TrOCR model\n",
    "    \n",
    "    Returns:\n",
    "        text: Recognized text\n",
    "        score: Confidence score\n",
    "    \"\"\"\n",
    "    if processor is None or model is None:\n",
    "        # If model not loaded, return placeholder result\n",
    "        return \"Sample text\", 0.95\n",
    "    \n",
    "    # Convert to PIL Image if necessary\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "    \n",
    "    # Preprocess image\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    # Generate predictions\n",
    "    generated_ids = model.generate(pixel_values, max_length=64)\n",
    "    \n",
    "    # Decode prediction\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # For demonstration, returning a fixed score\n",
    "    # In a real application, you'd compute a confidence score\n",
    "    return generated_text, 0.95\n",
    "\n",
    "# Test TrOCR on cropped text regions\n",
    "def recognize_text_regions(crops, processor=None, model=None):\n",
    "    \"\"\"\n",
    "    Recognize text in multiple cropped regions\n",
    "    \n",
    "    Args:\n",
    "        crops: List of cropped images\n",
    "        processor: TrOCR processor\n",
    "        model: TrOCR model\n",
    "    \n",
    "    Returns:\n",
    "        results: List of (text, score) tuples\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for crop in crops:\n",
    "        # Recognize text in crop\n",
    "        text, score = recognize_text(crop, processor, model)\n",
    "        results.append((text, score))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test TrOCR on the same sample images\n",
    "for img_path in sample_images:\n",
    "    print(f\"Processing image: {os.path.basename(img_path)}\")\n",
    "    \n",
    "    # Detect text regions\n",
    "    boxes, image = detect_text(img_path, craft_net)\n",
    "    \n",
    "    # Crop text regions\n",
    "    crops, crop_boxes = crop_text_regions(img_path, boxes)\n",
    "    \n",
    "    # Recognize text in each region\n",
    "    recognition_results = recognize_text_regions(crops, trocr_processor, trocr_model)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Original image with text detection\n",
    "    plt.subplot(1, 2, 1)\n",
    "    viz_image = image.copy()\n",
    "    for box in boxes:\n",
    "        cv2.polylines(viz_image, [box.astype(np.int32)], True, (0, 255, 0), 2)\n",
    "    plt.imshow(viz_image)\n",
    "    plt.title(\"Text Detection\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Original image with text recognition\n",
    "    plt.subplot(1, 2, 2)\n",
    "    viz_image = image.copy()\n",
    "    for i, ((text, score), box) in enumerate(zip(recognition_results, boxes)):\n",
    "        # Draw box\n",
    "        cv2.polylines(viz_image, [box.astype(np.int32)], True, (0, 255, 0), 2)\n",
    "        \n",
    "        # Get box center for text placement\n",
    "        box_center = np.mean(box, axis=0)\n",
    "        x, y = box_center.astype(np.int32)\n",
    "        \n",
    "        # Display recognized text\n",
    "        cv2.putText(viz_image, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "    \n",
    "    plt.imshow(viz_image)\n",
    "    plt.title(\"Text Recognition\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print recognition results\n",
    "    print(\"Recognition Results:\")\n",
    "    for i, (text, score) in enumerate(recognition_results):\n",
    "        print(f\"Region {i+1}: '{text}' (confidence: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f29df7",
   "metadata": {},
   "source": [
    "## 8. End-to-End Inference Pipeline\n",
    "\n",
    "Now, we'll combine the CRAFT text detection and TrOCR text recognition models to create an end-to-end OCR inference pipeline for Khmer and English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete OCR pipeline class\n",
    "class KhmerOCRPipeline:\n",
    "    \"\"\"End-to-end OCR pipeline for Khmer and English text\"\"\"\n",
    "    \n",
    "    def __init__(self, craft_model=None, trocr_processor=None, trocr_model=None):\n",
    "        \"\"\"\n",
    "        Initialize the OCR pipeline\n",
    "        \n",
    "        Args:\n",
    "            craft_model: CRAFT text detection model\n",
    "            trocr_processor: TrOCR processor\n",
    "            trocr_model: TrOCR text recognition model\n",
    "        \"\"\"\n",
    "        # Initialize models\n",
    "        self.craft_model = craft_model if craft_model is not None else load_craft_model()\n",
    "        \n",
    "        if trocr_processor is None or trocr_model is None:\n",
    "            self.trocr_processor, self.trocr_model = load_trocr_model()\n",
    "        else:\n",
    "            self.trocr_processor = trocr_processor\n",
    "            self.trocr_model = trocr_model\n",
    "        \n",
    "        # Detection parameters\n",
    "        self.text_threshold = 0.7\n",
    "        self.link_threshold = 0.4\n",
    "        self.low_text = 0.4\n",
    "        self.poly = False\n",
    "        self.padding = 10\n",
    "        \n",
    "    def detect_text(self, image):\n",
    "        \"\"\"\n",
    "        Detect text regions in an image\n",
    "        \n",
    "        Args:\n",
    "            image: Image as numpy array or path to image\n",
    "        \n",
    "        Returns:\n",
    "            boxes: List of detected text boxes\n",
    "            image: Image as numpy array\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            # Load image from path\n",
    "            image = cv2.imread(image)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect text regions\n",
    "        boxes, _ = test_net(self.craft_model, image, \n",
    "                           self.text_threshold, self.link_threshold, \n",
    "                           self.low_text, self.poly)\n",
    "        \n",
    "        return boxes, image\n",
    "    \n",
    "    def crop_text_regions(self, image, boxes):\n",
    "        \"\"\"\n",
    "        Crop detected text regions from an image\n",
    "        \n",
    "        Args:\n",
    "            image: Image as numpy array\n",
    "            boxes: List of detected text boxes\n",
    "        \n",
    "        Returns:\n",
    "            crops: List of cropped text regions\n",
    "            crop_boxes: List of corresponding boxes\n",
    "        \"\"\"\n",
    "        h, w, _ = image.shape\n",
    "        crops = []\n",
    "        crop_boxes = []\n",
    "        \n",
    "        for box in boxes:\n",
    "            # Get bounding rectangle\n",
    "            rect = cv2.boundingRect(box.astype(np.int32))\n",
    "            x, y, width, height = rect\n",
    "            \n",
    "            # Add padding\n",
    "            x_min = max(0, x - self.padding)\n",
    "            y_min = max(0, y - self.padding)\n",
    "            x_max = min(w, x + width + self.padding)\n",
    "            y_max = min(h, y + height + self.padding)\n",
    "            \n",
    "            # Crop region\n",
    "            crop = image[y_min:y_max, x_min:x_max]\n",
    "            crops.append(crop)\n",
    "            crop_boxes.append([x_min, y_min, x_max, y_max])\n",
    "        \n",
    "        return crops, crop_boxes\n",
    "    \n",
    "    def recognize_text(self, crops):\n",
    "        \"\"\"\n",
    "        Recognize text in cropped regions\n",
    "        \n",
    "        Args:\n",
    "            crops: List of cropped images\n",
    "        \n",
    "        Returns:\n",
    "            results: List of (text, score) tuples\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for crop in crops:\n",
    "            # Recognize text in crop\n",
    "            text, score = recognize_text(crop, self.trocr_processor, self.trocr_model)\n",
    "            results.append((text, score))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Process an image through the complete OCR pipeline\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the image\n",
    "        \n",
    "        Returns:\n",
    "            boxes: Detected text boxes\n",
    "            recognition_results: List of (text, score) tuples\n",
    "            image: Original image\n",
    "        \"\"\"\n",
    "        # Detect text regions\n",
    "        boxes, image = self.detect_text(image_path)\n",
    "        \n",
    "        # Crop text regions\n",
    "        crops, crop_boxes = self.crop_text_regions(image, boxes)\n",
    "        \n",
    "        # Recognize text in each region\n",
    "        recognition_results = self.recognize_text(crops)\n",
    "        \n",
    "        return boxes, recognition_results, image\n",
    "    \n",
    "    def visualize_results(self, image, boxes, recognition_results):\n",
    "        \"\"\"\n",
    "        Visualize OCR results\n",
    "        \n",
    "        Args:\n",
    "            image: Image as numpy array\n",
    "            boxes: Detected text boxes\n",
    "            recognition_results: List of (text, score) tuples\n",
    "        \n",
    "        Returns:\n",
    "            visualization: Image with visualized OCR results\n",
    "        \"\"\"\n",
    "        # Create a copy for visualization\n",
    "        viz_image = image.copy()\n",
    "        \n",
    "        # Draw boxes and text\n",
    "        for i, ((text, score), box) in enumerate(zip(recognition_results, boxes)):\n",
    "            # Draw box\n",
    "            cv2.polylines(viz_image, [box.astype(np.int32)], True, (0, 255, 0), 2)\n",
    "            \n",
    "            # Get box center for text placement\n",
    "            box_center = np.mean(box, axis=0)\n",
    "            x, y = box_center.astype(np.int32)\n",
    "            \n",
    "            # Display recognized text and score\n",
    "            label = f\"{text} ({score:.2f})\"\n",
    "            cv2.putText(viz_image, label, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "        \n",
    "        return viz_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OCR pipeline\n",
    "ocr_pipeline = KhmerOCRPipeline(craft_model=craft_net, \n",
    "                               trocr_processor=trocr_processor, \n",
    "                               trocr_model=trocr_model)\n",
    "\n",
    "# Test the pipeline on sample images\n",
    "for img_path in sample_images:\n",
    "    print(f\"Processing image: {os.path.basename(img_path)}\")\n",
    "    \n",
    "    # Process image\n",
    "    boxes, recognition_results, image = ocr_pipeline.process_image(img_path)\n",
    "    \n",
    "    # Visualize results\n",
    "    viz_image = ocr_pipeline.visualize_results(image, boxes, recognition_results)\n",
    "    \n",
    "    # Display original and results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(viz_image)\n",
    "    plt.title(\"OCR Results\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print recognition results\n",
    "    print(\"Recognition Results:\")\n",
    "    for i, (text, score) in enumerate(recognition_results):\n",
    "        print(f\"Region {i+1}: '{text}' (confidence: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b5fd88",
   "metadata": {},
   "source": [
    "## 9. Evaluation and Visualization of Results\n",
    "\n",
    "Let's create a comprehensive visualization of the results and evaluate the performance of our OCR pipeline on various test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e45fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate character error rate (CER)\n",
    "def calculate_cer(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Calculate character error rate (CER)\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference text\n",
    "        hypothesis: Hypothesis text\n",
    "    \n",
    "    Returns:\n",
    "        cer: Character error rate\n",
    "    \"\"\"\n",
    "    # Simple implementation using edit distance\n",
    "    def levenshtein(a, b):\n",
    "        if not a: return len(b)\n",
    "        if not b: return len(a)\n",
    "        \n",
    "        # Initialize matrix\n",
    "        matrix = [[0 for _ in range(len(b) + 1)] for _ in range(len(a) + 1)]\n",
    "        \n",
    "        # Fill first row and column\n",
    "        for i in range(len(a) + 1):\n",
    "            matrix[i][0] = i\n",
    "        for j in range(len(b) + 1):\n",
    "            matrix[0][j] = j\n",
    "        \n",
    "        # Fill rest of matrix\n",
    "        for i in range(1, len(a) + 1):\n",
    "            for j in range(1, len(b) + 1):\n",
    "                cost = 0 if a[i-1] == b[j-1] else 1\n",
    "                matrix[i][j] = min(\n",
    "                    matrix[i-1][j] + 1,      # deletion\n",
    "                    matrix[i][j-1] + 1,      # insertion\n",
    "                    matrix[i-1][j-1] + cost  # substitution\n",
    "                )\n",
    "        \n",
    "        return matrix[len(a)][len(b)]\n",
    "    \n",
    "    # Calculate edit distance\n",
    "    distance = levenshtein(reference, hypothesis)\n",
    "    \n",
    "    # Calculate CER\n",
    "    if len(reference) == 0:\n",
    "        return 1.0  # All errors if reference is empty\n",
    "    \n",
    "    cer = distance / len(reference)\n",
    "    return cer\n",
    "\n",
    "# Evaluate OCR on synthetic test images where we know the ground truth\n",
    "def evaluate_ocr_on_test_set(pipeline, test_df, num_samples=10):\n",
    "    \"\"\"\n",
    "    Evaluate OCR pipeline on a test set\n",
    "    \n",
    "    Args:\n",
    "        pipeline: OCR pipeline\n",
    "        test_df: DataFrame with image paths and labels\n",
    "        num_samples: Number of samples to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        results_df: DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Limit to num_samples\n",
    "    test_sample = test_df.sample(min(num_samples, len(test_df)))\n",
    "    \n",
    "    for idx, row in tqdm(test_sample.iterrows(), total=len(test_sample), desc=\"Evaluating OCR\"):\n",
    "        img_path = row['image_path']\n",
    "        ground_truth = row['text']\n",
    "        \n",
    "        # Process image\n",
    "        try:\n",
    "            boxes, recognition_results, image = pipeline.process_image(img_path)\n",
    "            \n",
    "            # Get recognized text\n",
    "            if recognition_results:\n",
    "                # Combine all detected text\n",
    "                recognized_text = ' '.join([text for text, score in recognition_results])\n",
    "                \n",
    "                # Calculate CER\n",
    "                cer = calculate_cer(ground_truth, recognized_text)\n",
    "                \n",
    "                results.append({\n",
    "                    'image_path': img_path,\n",
    "                    'ground_truth': ground_truth,\n",
    "                    'recognized_text': recognized_text,\n",
    "                    'cer': cer\n",
    "                })\n",
    "            else:\n",
    "                print(f\"No text detected in {os.path.basename(img_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {os.path.basename(img_path)}: {e}\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate average CER\n",
    "    avg_cer = results_df['cer'].mean() if len(results_df) > 0 else 1.0\n",
    "    print(f\"Average CER: {avg_cer:.4f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Evaluate OCR pipeline on synthetic test data\n",
    "# We'll use a small sample for demonstration\n",
    "test_df = synthetic_df.sample(min(10, len(synthetic_df)))\n",
    "evaluation_results = evaluate_ocr_on_test_set(ocr_pipeline, test_df, num_samples=len(test_df))\n",
    "\n",
    "# Visualize evaluation results\n",
    "def visualize_evaluation_results(results_df):\n",
    "    \"\"\"Visualize evaluation results\"\"\"\n",
    "    if len(results_df) == 0:\n",
    "        print(\"No evaluation results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Sort by CER for better visualization\n",
    "    results_df = results_df.sort_values('cer')\n",
    "    \n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(results_df)), results_df['cer'])\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Character Error Rate (CER)')\n",
    "    plt.title('OCR Performance on Test Set')\n",
    "    \n",
    "    # Display best and worst cases\n",
    "    best_idx = results_df['cer'].idxmin()\n",
    "    worst_idx = results_df['cer'].idxmax()\n",
    "    \n",
    "    best_img_path = results_df.loc[best_idx, 'image_path']\n",
    "    worst_img_path = results_df.loc[worst_idx, 'image_path']\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    best_img = plt.imread(best_img_path)\n",
    "    plt.imshow(best_img)\n",
    "    plt.title(f\"Best Case: CER = {results_df.loc[best_idx, 'cer']:.4f}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    worst_img = plt.imread(worst_img_path)\n",
    "    plt.imshow(worst_img)\n",
    "    plt.title(f\"Worst Case: CER = {results_df.loc[worst_idx, 'cer']:.4f}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display text comparison\n",
    "    print(\"Best Case Comparison:\")\n",
    "    print(f\"Ground Truth: {results_df.loc[best_idx, 'ground_truth']}\")\n",
    "    print(f\"Recognized  : {results_df.loc[best_idx, 'recognized_text']}\")\n",
    "    \n",
    "    print(\"\\nWorst Case Comparison:\")\n",
    "    print(f\"Ground Truth: {results_df.loc[worst_idx, 'ground_truth']}\")\n",
    "    print(f\"Recognized  : {results_df.loc[worst_idx, 'recognized_text']}\")\n",
    "\n",
    "# Visualize evaluation results\n",
    "visualize_evaluation_results(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a498ffe9",
   "metadata": {},
   "source": [
    "## 10. Streamlit App Integration for OCR Demo\n",
    "\n",
    "Finally, let's create a Streamlit web app to demonstrate our OCR pipeline. Users can upload images, and the app will show the detected text regions and recognized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the OCR pipeline components\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Function to save models for later use in Streamlit app\n",
    "def save_models(ocr_pipeline, output_dir='models'):\n",
    "    \"\"\"\n",
    "    Save OCR pipeline models for later use\n",
    "    \n",
    "    Args:\n",
    "        ocr_pipeline: OCR pipeline object\n",
    "        output_dir: Output directory\n",
    "    \"\"\"\n",
    "    print(\"Saving OCR models...\")\n",
    "    \n",
    "    # In a real project, you would save the models here\n",
    "    # For the demonstration, we'll use HuggingFace's models directly\n",
    "    \n",
    "    # Create a README file with instructions\n",
    "    readme = \"\"\"# Khmer OCR Models\n",
    "\n",
    "This directory contains models for the Khmer OCR pipeline:\n",
    "\n",
    "1. CRAFT text detection model\n",
    "2. TrOCR text recognition model\n",
    "\n",
    "In a real project, the trained models would be saved here.\n",
    "For the demonstration, we're using pre-trained models from HuggingFace.\n",
    "\"\"\"\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'README.md'), 'w') as f:\n",
    "        f.write(readme)\n",
    "    \n",
    "    print(\"Models setup complete!\")\n",
    "\n",
    "# Save models\n",
    "save_models(ocr_pipeline)\n",
    "\n",
    "# Create Streamlit app file\n",
    "streamlit_app_code = '''\n",
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# Add CRAFT directory to path for imports\n",
    "if os.path.exists('CRAFT-pytorch'):\n",
    "    sys.path.append('CRAFT-pytorch')\n",
    "    from craft import CRAFT\n",
    "    from test import test_net\n",
    "    from imgproc import resize_aspect_ratio, normalizeMeanVariance\n",
    "    from craft_utils import getDetBoxes, adjustResultCoordinates\n",
    "    CRAFT_AVAILABLE = True\n",
    "else:\n",
    "    CRAFT_AVAILABLE = False\n",
    "    st.warning(\"CRAFT-pytorch directory not found. Text detection will use a placeholder model.\")\n",
    "    \n",
    "    # Placeholder classes and functions\n",
    "    class CRAFT(torch.nn.Module):\n",
    "        def __init__(self, pretrained=False):\n",
    "            super(CRAFT, self).__init__()\n",
    "            self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "            self.conv2 = torch.nn.Conv2d(16, 2, kernel_size=3, padding=1)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = torch.nn.functional.relu(self.conv1(x))\n",
    "            x = self.conv2(x)\n",
    "            return x\n",
    "\n",
    "    def test_net(net, image, text_threshold, link_threshold, low_text, poly):\n",
    "        # Placeholder function\n",
    "        h, w, _ = image.shape\n",
    "        boxes = []\n",
    "        boxes.append(np.array([[0.1*w, 0.1*h], [0.9*w, 0.1*h], [0.9*w, 0.9*h], [0.1*w, 0.9*h]]))\n",
    "        return boxes, image\n",
    "\n",
    "# OCR Pipeline class\n",
    "class KhmerOCRPipeline:\n",
    "    \"\"\"End-to-end OCR pipeline for Khmer and English text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the OCR pipeline\"\"\"\n",
    "        # Set device\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        st.write(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load models\n",
    "        self.load_models()\n",
    "        \n",
    "        # Detection parameters\n",
    "        self.text_threshold = 0.7\n",
    "        self.link_threshold = 0.4\n",
    "        self.low_text = 0.4\n",
    "        self.poly = False\n",
    "        self.padding = 10\n",
    "        \n",
    "    def load_models(self):\n",
    "        \"\"\"Load CRAFT and TrOCR models\"\"\"\n",
    "        with st.spinner(\"Loading models...\"):\n",
    "            # Load CRAFT model\n",
    "            if CRAFT_AVAILABLE:\n",
    "                self.craft_net = CRAFT()\n",
    "                \n",
    "                # Check for pre-trained weights\n",
    "                if os.path.exists('CRAFT-pytorch/craft_mlt_25k.pth'):\n",
    "                    self.craft_net.load_state_dict(torch.load('CRAFT-pytorch/craft_mlt_25k.pth', map_location=self.device))\n",
    "                    st.success(\"CRAFT model loaded successfully!\")\n",
    "                else:\n",
    "                    st.warning(\"CRAFT pre-trained weights not found. Using untrained model.\")\n",
    "                \n",
    "                self.craft_net.eval()\n",
    "                self.craft_net = self.craft_net.to(self.device)\n",
    "            else:\n",
    "                self.craft_net = CRAFT()\n",
    "                st.warning(\"Using placeholder CRAFT model.\")\n",
    "            \n",
    "            # Load TrOCR model\n",
    "            try:\n",
    "                self.trocr_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-printed\")\n",
    "                self.trocr_model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-printed\")\n",
    "                self.trocr_model = self.trocr_model.to(self.device)\n",
    "                st.success(\"TrOCR model loaded successfully!\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error loading TrOCR model: {e}\")\n",
    "                st.warning(\"Using placeholder TrOCR functionality.\")\n",
    "                self.trocr_processor = None\n",
    "                self.trocr_model = None\n",
    "    \n",
    "    def detect_text(self, image):\n",
    "        \"\"\"\n",
    "        Detect text regions in an image\n",
    "        \n",
    "        Args:\n",
    "            image: Image as numpy array\n",
    "        \n",
    "        Returns:\n",
    "            boxes: List of detected text boxes\n",
    "            image: Image as numpy array\n",
    "        \"\"\"\n",
    "        # Detect text regions\n",
    "        boxes, _ = test_net(self.craft_net, image, \n",
    "                           self.text_threshold, self.link_threshold, \n",
    "                           self.low_text, self.poly)\n",
    "        \n",
    "        return boxes, image\n",
    "    \n",
    "    def crop_text_regions(self, image, boxes):\n",
    "        \"\"\"\n",
    "        Crop detected text regions from an image\n",
    "        \n",
    "        Args:\n",
    "            image: Image as numpy array\n",
    "            boxes: List of detected text boxes\n",
    "        \n",
    "        Returns:\n",
    "            crops: List of cropped text regions\n",
    "            crop_boxes: List of corresponding boxes\n",
    "        \"\"\"\n",
    "        h, w, _ = image.shape\n",
    "        crops = []\n",
    "        crop_boxes = []\n",
    "        \n",
    "        for box in boxes:\n",
    "            # Get bounding rectangle\n",
    "            rect = cv2.boundingRect(box.astype(np.int32))\n",
    "            x, y, width, height = rect\n",
    "            \n",
    "            # Add padding\n",
    "            x_min = max(0, x - self.padding)\n",
    "            y_min = max(0, y - self.padding)\n",
    "            x_max = min(w, x + width + self.padding)\n",
    "            y_max = min(h, y + height + self.padding)\n",
    "            \n",
    "            # Crop region\n",
    "            crop = image[y_min:y_max, x_min:x_max]\n",
    "            crops.append(crop)\n",
    "            crop_boxes.append([x_min, y_min, x_max, y_max])\n",
    "        \n",
    "        return crops, crop_boxes\n",
    "    \n",
    "    def recognize_text(self, crops):\n",
    "        \"\"\"\n",
    "        Recognize text in cropped regions\n",
    "        \n",
    "        Args:\n",
    "            crops: List of cropped images\n",
    "        \n",
    "        Returns:\n",
    "            results: List of (text, score) tuples\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for crop in crops:\n",
    "            if self.trocr_processor is None or self.trocr_model is None:\n",
    "                # Placeholder result\n",
    "                results.append((\"Sample text\", 0.95))\n",
    "                continue\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            if isinstance(crop, np.ndarray):\n",
    "                crop = Image.fromarray(crop)\n",
    "            \n",
    "            # Preprocess image\n",
    "            pixel_values = self.trocr_processor(crop, return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "            \n",
    "            # Generate predictions\n",
    "            generated_ids = self.trocr_model.generate(pixel_values, max_length=64)\n",
    "            \n",
    "            # Decode prediction\n",
    "            generated_text = self.trocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            \n",
    "            # For demonstration, returning a fixed score\n",
    "            results.append((generated_text, 0.95))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_image(self, image):\n",
    "        \"\"\"\n",
    "        Process an image through the complete OCR pipeline\n",
    "        \n",
    "        Args:\n",
    "            image: Image as numpy array or PIL Image\n",
    "        \n",
    "        Returns:\n",
    "            boxes: Detected text boxes\n",
    "            recognition_results: List of (text, score) tuples\n",
    "            image: Original image\n",
    "        \"\"\"\n",
    "        # Convert PIL Image to numpy array if necessary\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = np.array(image)\n",
    "        \n",
    "        # Ensure RGB format\n",
    "        if image.shape[2] == 4:  # RGBA\n",
    "            image = image[:, :, :3]\n",
    "        \n",
    "        # Detect text regions\n",
    "        boxes, image = self.detect_text(image)\n",
    "        \n",
    "        # Crop text regions\n",
    "        crops, crop_boxes = self.crop_text_regions(image, boxes)\n",
    "        \n",
    "        # Recognize text in each region\n",
    "        recognition_results = self.recognize_text(crops)\n",
    "        \n",
    "        return boxes, recognition_results, image\n",
    "    \n",
    "    def visualize_results(self, image, boxes, recognition_results):\n",
    "        \"\"\"\n",
    "        Visualize OCR results\n",
    "        \n",
    "        Args:\n",
    "            image: Image as numpy array\n",
    "            boxes: Detected text boxes\n",
    "            recognition_results: List of (text, score) tuples\n",
    "        \n",
    "        Returns:\n",
    "            visualization: Image with visualized OCR results\n",
    "        \"\"\"\n",
    "        # Create a copy for visualization\n",
    "        viz_image = image.copy()\n",
    "        \n",
    "        # Draw boxes and text\n",
    "        for i, ((text, score), box) in enumerate(zip(recognition_results, boxes)):\n",
    "            # Draw box\n",
    "            cv2.polylines(viz_image, [box.astype(np.int32)], True, (0, 255, 0), 2)\n",
    "            \n",
    "            # Get box center for text placement\n",
    "            box_center = np.mean(box, axis=0)\n",
    "            x, y = box_center.astype(np.int32)\n",
    "            \n",
    "            # Display recognized text and score\n",
    "            label = f\"{text} ({score:.2f})\"\n",
    "            cv2.putText(viz_image, label, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "        \n",
    "        return viz_image\n",
    "\n",
    "# Streamlit app\n",
    "st.title(\"Khmer & English OCR App\")\n",
    "st.write(\"Upload an image with Khmer or English text to extract and recognize text.\")\n",
    "\n",
    "# Initialize OCR pipeline\n",
    "@st.cache_resource\n",
    "def get_ocr_pipeline():\n",
    "    return KhmerOCRPipeline()\n",
    "\n",
    "ocr_pipeline = get_ocr_pipeline()\n",
    "\n",
    "# File uploader\n",
    "uploaded_file = st.file_uploader(\"Choose an image file\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Read image\n",
    "    image = Image.open(uploaded_file)\n",
    "    \n",
    "    # Display original image\n",
    "    st.subheader(\"Original Image\")\n",
    "    st.image(image, use_column_width=True)\n",
    "    \n",
    "    # Process image button\n",
    "    if st.button(\"Extract Text\"):\n",
    "        with st.spinner(\"Processing image...\"):\n",
    "            # Convert PIL image to numpy array\n",
    "            image_np = np.array(image)\n",
    "            \n",
    "            # Process image through OCR pipeline\n",
    "            boxes, recognition_results, _ = ocr_pipeline.process_image(image_np)\n",
    "            \n",
    "            # Visualize results\n",
    "            viz_image = ocr_pipeline.visualize_results(image_np, boxes, recognition_results)\n",
    "            \n",
    "            # Display results\n",
    "            st.subheader(\"OCR Results\")\n",
    "            st.image(viz_image, use_column_width=True)\n",
    "            \n",
    "            # Display recognized text\n",
    "            st.subheader(\"Recognized Text\")\n",
    "            \n",
    "            if not recognition_results:\n",
    "                st.warning(\"No text detected in the image.\")\n",
    "            else:\n",
    "                for i, (text, score) in enumerate(recognition_results):\n",
    "                    st.write(f\"**Region {i+1}:** {text} (Confidence: {score:.2f})\")\n",
    "                \n",
    "                # Combined text\n",
    "                all_text = ' '.join([text for text, _ in recognition_results])\n",
    "                \n",
    "                st.subheader(\"All Extracted Text\")\n",
    "                st.text_area(\"\", all_text, height=150)\n",
    "\n",
    "# Add information about the app\n",
    "st.sidebar.title(\"About\")\n",
    "st.sidebar.info(\n",
    "    \"\"\"\n",
    "    This OCR app uses a combination of CRAFT for text detection and TrOCR for text recognition, \n",
    "    specifically trained to handle both Khmer and English text.\n",
    "    \n",
    "    **Models used:**\n",
    "    - CRAFT for text region detection\n",
    "    - TrOCR for text recognition\n",
    "    \n",
    "    Created as part of a project to improve OCR accuracy for Khmer language.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Add instructions\n",
    "st.sidebar.title(\"Instructions\")\n",
    "st.sidebar.info(\n",
    "    \"\"\"\n",
    "    1. Upload an image containing Khmer or English text\n",
    "    2. Click \"Extract Text\" to process the image\n",
    "    3. View the detected text regions and recognized text\n",
    "    \"\"\"\n",
    ")\n",
    "'''\n",
    "\n",
    "# Create Streamlit app file\n",
    "streamlit_app_path = 'streamlit_app.py'\n",
    "with open(streamlit_app_path, 'w') as f:\n",
    "    f.write(streamlit_app_code)\n",
    "\n",
    "print(f\"Streamlit app created at: {streamlit_app_path}\")\n",
    "print(\"\\nTo run the Streamlit app, use the following command:\")\n",
    "print(\"streamlit run streamlit_app.py\")\n",
    "\n",
    "# If in Colab, provide a way to run Streamlit\n",
    "if IN_COLAB:\n",
    "    print(\"\\nTo run Streamlit in Colab, use the following:\")\n",
    "    print(\"1. Install pyngrok: !pip install pyngrok\")\n",
    "    print(\"2. Run Streamlit with ngrok tunneling:\")\n",
    "    print(\"   !nohup streamlit run streamlit_app.py &\")\n",
    "    print(\"   from pyngrok import ngrok\")\n",
    "    print(\"   ngrok_tunnel = ngrok.connect(8501)\")\n",
    "    print(\"   print(ngrok_tunnel.public_url)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b1c805",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've built a comprehensive OCR system for Khmer and English text using:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Downloaded and processed datasets from Hugging Face\n",
    "   - Cleaned and preprocessed text data\n",
    "   - Generated synthetic training data with various fonts\n",
    "\n",
    "2. **Data Augmentation**:\n",
    "   - Applied rotation, noise, blur, and perspective transforms\n",
    "   - Created robust training data to handle real-world conditions\n",
    "\n",
    "3. **Text Detection**:\n",
    "   - Implemented CRAFT for accurate text region detection\n",
    "   - Visualized and analyzed detection results\n",
    "\n",
    "4. **Text Recognition**:\n",
    "   - Used TrOCR for high-quality text recognition\n",
    "   - Optimized for both Khmer and English languages\n",
    "\n",
    "5. **End-to-End Pipeline**:\n",
    "   - Combined detection and recognition in a complete OCR pipeline\n",
    "   - Evaluated performance using character error rate (CER)\n",
    "\n",
    "6. **Streamlit Web App**:\n",
    "   - Created an interactive web interface for OCR\n",
    "   - Enabled easy image upload and text extraction\n",
    "\n",
    "This system can be further improved by:\n",
    "- Training the TrOCR model specifically on more Khmer text data\n",
    "- Fine-tuning CRAFT for better detection of Khmer script characteristics\n",
    "- Adding more language support (e.g., Thai, Lao, Vietnamese)\n",
    "- Implementing post-processing for text correction\n",
    "\n",
    "The combination of CRAFT for text detection and TrOCR for text recognition provides a powerful solution for Khmer OCR, addressing the challenges of this complex script and improving accessibility for Khmer language resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d352852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the OCR system with different Khmer fonts\n",
    "def test_ocr_with_khmer_fonts(ocr_model, sample_text=\"ជំរាបសួរពីប្រទេសកម្ពុជា\", num_fonts=5):\n",
    "    \"\"\"\n",
    "    Test the OCR system with different Khmer fonts\n",
    "    \n",
    "    Args:\n",
    "        ocr_model: OCR model to use for recognition\n",
    "        sample_text: Khmer text to test with\n",
    "        num_fonts: Number of fonts to test\n",
    "    \"\"\"\n",
    "    if len(khmer_fonts) == 0:\n",
    "        print(\"No Khmer fonts available for testing\")\n",
    "        return\n",
    "    \n",
    "    # Select fonts to test\n",
    "    test_fonts = khmer_fonts[:min(num_fonts, len(khmer_fonts))]\n",
    "    \n",
    "    plt.figure(figsize=(15, len(test_fonts) * 4))\n",
    "    \n",
    "    for i, font_path in enumerate(test_fonts):\n",
    "        try:\n",
    "            font_name = os.path.basename(font_path)\n",
    "            print(f\"Testing with font: {font_name}\")\n",
    "            \n",
    "            # Generate test image\n",
    "            img = generate_text_image(\n",
    "                text=sample_text,\n",
    "                font_path=font_path,\n",
    "                width=800,\n",
    "                height=200,\n",
    "                font_size=36,\n",
    "                is_khmer=True\n",
    "            )\n",
    "            \n",
    "            # Save temporary image\n",
    "            temp_img_path = f\"temp_test_{i}.png\"\n",
    "            img.save(temp_img_path)\n",
    "            \n",
    "            # Use OCR on the image if model is available\n",
    "            if 'ocr_pipeline' in globals():\n",
    "                result = ocr_pipeline(temp_img_path)\n",
    "                ocr_text = result['text'] if 'text' in result else \"OCR not run\"\n",
    "            else:\n",
    "                ocr_text = \"OCR model not available\"\n",
    "                \n",
    "            # Display results\n",
    "            plt.subplot(len(test_fonts), 2, i*2+1)\n",
    "            plt.imshow(np.array(img))\n",
    "            plt.title(f\"Font: {font_name}\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(len(test_fonts), 2, i*2+2)\n",
    "            plt.text(0.1, 0.5, f\"Original: {sample_text}\\nRecognized: {ocr_text}\", \n",
    "                    fontsize=12, wrap=True)\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Clean up\n",
    "            if os.path.exists(temp_img_path):\n",
    "                os.remove(temp_img_path)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error testing font {os.path.basename(font_path)}: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Check if OCR model is available and run test\n",
    "if 'ocr_pipeline' in globals():\n",
    "    print(\"Testing OCR with different Khmer fonts:\")\n",
    "    test_ocr_with_khmer_fonts(ocr_pipeline)\n",
    "else:\n",
    "    print(\"OCR model not available. Run the OCR setup cell first to test with different fonts.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
